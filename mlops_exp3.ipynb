{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyMjm4tWm1dSko0pUeBndY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Experiment 3\n","Comparison of Decision Tree and Random Forest Classifiers\n","\n","## Aim\n","To implement and compare Decision Tree and Random Forest classifiers on the Wine dataset and evaluate their performance based on classification metrics.\n","\n","## Objectives\n","1. To understand the Wine dataset and perform basic exploratory data analysis (EDA).\n","2. To preprocess and split the data into training and testing sets.\n","3. To implement Decision Tree and Random Forest classifiers for predicting wine categories.\n","4. To analyze and compare model performance using accuracy, confusion matrices, and classification reports.\n","5. To gain insights into the effectiveness of ensemble learning compared to individual decision trees.\n","\n","## Course Outcomes\n","1. Conduct exploratory data analysis on structured datasets using Python.\n","2. Implement and train Decision Tree and Random Forest classifiers.\n","3. Understand the differences between individual decision trees and ensemble methods.\n","4. Evaluate classifier performance using metrics such as accuracy, precision, recall, and confusion matrices.\n","5. Develop a deeper understanding of classification algorithms and their real-world applications.\n","\n","## Theory\n","- Wine Dataset: The Wine dataset from `scikit-learn` is a well-known dataset used for classification tasks. It consists of 178 samples belonging to three different wine classes. Each sample has 13 features describing various chemical properties of the wine, including alcohol content, flavonoids, and color intensity.\n","- Decision Tree Classifier: A Decision Tree classifier is a supervised learning algorithm that uses a tree-like structure for decision-making. The model splits the dataset based on feature values, recursively forming branches that ultimately lead to class predictions. It can suffer from overfitting, particularly with deep trees.\n","- Random Forest Classifier: Random Forest is an ensemble learning method that builds multiple Decision Trees and combines their predictions to improve accuracy. It helps mitigate the overfitting issue seen in single decision trees and generally performs better in classification tasks.\n","\n","Machine Learning Steps:\n","1. Data Exploration: Understanding the dataset structure and distributions.\n","2. Data Preprocessing: Cleaning and preparing the dataset.\n","3. Model Training: Applying classification algorithms to learn patterns.\n","4. Model Evaluation: Assessing classifier performance using accuracy and confusion matrices.\n","\n","## Procedure\n","\n","1. Load the Dataset\n","    - Load the Wine dataset using `load_wine()`.\n","    - Convert it into a pandas DataFrame.\n","    - Separate features (`X`) and target labels (`y`).\n","\n","2. Perform Exploratory Data Analysis\n","    - Display the first few rows using `head()`.\n","    - Retrieve dataset information using `info()` and `describe()`.\n","    - Visualize class distribution using `value_counts()`.\n","\n","3. Data Preprocessing\n","    - Check for missing values or outliers (if necessary).\n","    - In case of missing values, apply techniques such as mean imputation.\n","\n","4. Split the Data: Divide the dataset into training and testing sets using `train_test_split()`, with an 80-20 split.\n","\n","5. Train the Decision Tree Classifier\n","    - Initialize `DecisionTreeClassifier()`.\n","    - Train the classifier using `fit()` on training data.\n","\n","6. Train the Random Forest Classifier\n","    - Initialize `RandomForestClassifier()` with `n_estimators=100`.\n","    - Train the classifier using `fit()` on training data.\n","\n","7. Model Predictions: Use both models to predict class labels on test data.\n","\n","8. Evaluate the Models\n","    - Compute accuracy scores for both models.\n","    - Generate confusion matrices and classification reports.\n","\n","9. Compare Model Performances: Analyze results to determine which classifier performs better.\n","\n","## Results\n","\n","- Exploratory Data Analysis\n","    - The dataset consists of 178 samples with 13 features.\n","    - Class distributions are relatively balanced.\n","    - No missing values were found, ensuring data integrity.\n","\n","- Model Training and Evaluation\n","    - Decision Tree Classifier\n","      - Accuracy: Approximately **87%**.\n","      - Classification Report: Precision and recall varied across classes.\n","      - Confusion Matrix: Some misclassifications were observed, likely due to overfitting.\n","\n","    - Random Forest Classifier\n","        - Accuracy: Approximately **95%**.\n","        - Classification Report: Higher precision and recall compared to Decision Tree.\n","        - Confusion Matrix: Fewer misclassifications compared to the Decision Tree classifier.\n","\n","- Model Comparison\n","    - The **Random Forest classifier outperformed the Decision Tree** in terms of accuracy.\n","    - The ensemble technique reduced variance and improved generalization.\n","    - Decision Trees tended to overfit, leading to slightly lower test accuracy.\n","\n","## Conclusion\n","The Random Forest classifier demonstrated superior performance compared to the Decision Tree classifier. The ensemble method mitigated overfitting while improving accuracy. This study highlights the importance of ensemble learning in machine learning tasks and reinforces the effectiveness of model evaluation metrics."],"metadata":{"id":"S__9ObAi_dMn"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","from sklearn.datasets import load_wine # Using the wine dataset\n","\n","# Load the wine dataset\n","wine = load_wine()\n","data = pd.DataFrame(data= np.c_[wine['data'], wine['target']],\n","                     columns= wine['feature_names'] + ['target'])\n","\n","print(data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9e-razH_hrB","executionInfo":{"status":"ok","timestamp":1745488406481,"user_tz":-330,"elapsed":15057,"user":{"displayName":"Mandar Bakshi","userId":"03609835573555259268"}},"outputId":"2e09d82c-18f6-4cc0-8d9c-576172e2c319"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n","0    14.23        1.71  2.43               15.6      127.0           2.80   \n","1    13.20        1.78  2.14               11.2      100.0           2.65   \n","2    13.16        2.36  2.67               18.6      101.0           2.80   \n","3    14.37        1.95  2.50               16.8      113.0           3.85   \n","4    13.24        2.59  2.87               21.0      118.0           2.80   \n","\n","   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n","0        3.06                  0.28             2.29             5.64  1.04   \n","1        2.76                  0.26             1.28             4.38  1.05   \n","2        3.24                  0.30             2.81             5.68  1.03   \n","3        3.49                  0.24             2.18             7.80  0.86   \n","4        2.69                  0.39             1.82             4.32  1.04   \n","\n","   od280/od315_of_diluted_wines  proline  target  \n","0                          3.92   1065.0     0.0  \n","1                          3.40   1050.0     0.0  \n","2                          3.17   1185.0     0.0  \n","3                          3.45   1480.0     0.0  \n","4                          2.93    735.0     0.0  \n"]}]},{"cell_type":"code","source":["print(data.info())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wsvdeZKjIc6C","executionInfo":{"status":"ok","timestamp":1745488406493,"user_tz":-330,"elapsed":8,"user":{"displayName":"Mandar Bakshi","userId":"03609835573555259268"}},"outputId":"378f16d0-a5df-4b12-e0d1-6b02d4b8d68e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 178 entries, 0 to 177\n","Data columns (total 14 columns):\n"," #   Column                        Non-Null Count  Dtype  \n","---  ------                        --------------  -----  \n"," 0   alcohol                       178 non-null    float64\n"," 1   malic_acid                    178 non-null    float64\n"," 2   ash                           178 non-null    float64\n"," 3   alcalinity_of_ash             178 non-null    float64\n"," 4   magnesium                     178 non-null    float64\n"," 5   total_phenols                 178 non-null    float64\n"," 6   flavanoids                    178 non-null    float64\n"," 7   nonflavanoid_phenols          178 non-null    float64\n"," 8   proanthocyanins               178 non-null    float64\n"," 9   color_intensity               178 non-null    float64\n"," 10  hue                           178 non-null    float64\n"," 11  od280/od315_of_diluted_wines  178 non-null    float64\n"," 12  proline                       178 non-null    float64\n"," 13  target                        178 non-null    float64\n","dtypes: float64(14)\n","memory usage: 19.6 KB\n","None\n"]}]},{"cell_type":"code","source":["print(data.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tot4QGkIh73","executionInfo":{"status":"ok","timestamp":1745488406684,"user_tz":-330,"elapsed":190,"user":{"displayName":"Mandar Bakshi","userId":"03609835573555259268"}},"outputId":"77e5232f-44a0-4a62-d492-9b06eb74d32e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n","count  178.000000  178.000000  178.000000         178.000000  178.000000   \n","mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n","std      0.811827    1.117146    0.274344           3.339564   14.282484   \n","min     11.030000    0.740000    1.360000          10.600000   70.000000   \n","25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n","50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n","75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n","max     14.830000    5.800000    3.230000          30.000000  162.000000   \n","\n","       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n","count     178.000000  178.000000            178.000000       178.000000   \n","mean        2.295112    2.029270              0.361854         1.590899   \n","std         0.625851    0.998859              0.124453         0.572359   \n","min         0.980000    0.340000              0.130000         0.410000   \n","25%         1.742500    1.205000              0.270000         1.250000   \n","50%         2.355000    2.135000              0.340000         1.555000   \n","75%         2.800000    2.875000              0.437500         1.950000   \n","max         3.880000    5.080000              0.660000         3.580000   \n","\n","       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n","count       178.000000  178.000000                    178.000000   178.000000   \n","mean          5.058090    0.957449                      2.611685   746.893258   \n","std           2.318286    0.228572                      0.709990   314.907474   \n","min           1.280000    0.480000                      1.270000   278.000000   \n","25%           3.220000    0.782500                      1.937500   500.500000   \n","50%           4.690000    0.965000                      2.780000   673.500000   \n","75%           6.200000    1.120000                      3.170000   985.000000   \n","max          13.000000    1.710000                      4.000000  1680.000000   \n","\n","           target  \n","count  178.000000  \n","mean     0.938202  \n","std      0.775035  \n","min      0.000000  \n","25%      0.000000  \n","50%      1.000000  \n","75%      2.000000  \n","max      2.000000  \n"]}]},{"cell_type":"code","source":["print(data['target'].value_counts()) # Class distribution\n","\n","# Data preprocessing (if needed) - Check for missing values, outliers, etc.\n","# In this case, the wine dataset is generally clean, but might need this for others.\n","# Example: Handling missing values (if any)\n","# data.fillna(data.mean(), inplace=True)\n","\n","# Split data into features (X) and target (y)\n","X = data.drop('target', axis=1)\n","y = data['target']\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","\n","# Decision Tree Classifier\n","dt_classifier = DecisionTreeClassifier(random_state=42) # You can tune hyperparameters here\n","dt_classifier.fit(X_train, y_train)\n","dt_predictions = dt_classifier.predict(X_test)\n","\n","# Random Forest Classifier\n","rf_classifier = RandomForestClassifier(random_state=42, n_estimators=100) # n_estimators is the number of trees\n","rf_classifier.fit(X_train, y_train)\n","rf_predictions = rf_classifier.predict(X_test)\n","\n","\n","# Evaluate the models\n","print(\"\\nDecision Tree Classifier:\")\n","print(f\"Accuracy: {accuracy_score(y_test, dt_predictions)}\")\n","print(classification_report(y_test, dt_predictions))\n","print(confusion_matrix(y_test, dt_predictions))\n","\n","\n","print(\"\\nRandom Forest Classifier:\")\n","print(f\"Accuracy: {accuracy_score(y_test, rf_predictions)}\")\n","print(classification_report(y_test, rf_predictions))\n","print(confusion_matrix(y_test, rf_predictions))\n","\n","# Compare the models (You can add more detailed comparisons here)\n","print(\"\\nModel Comparison:\")\n","if accuracy_score(y_test, dt_predictions) > accuracy_score(y_test, rf_predictions):\n","    print(\"Decision Tree performed slightly better.\")\n","else:\n","    print(\"Random Forest performed slightly better (or equally).\")\n"],"metadata":{"id":"yt1PfbBfImIE","executionInfo":{"status":"ok","timestamp":1745488407710,"user_tz":-330,"elapsed":1023,"user":{"displayName":"Mandar Bakshi","userId":"03609835573555259268"}},"outputId":"5a0bf935-0228-41ea-922e-1c08a7f0d878","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["target\n","1.0    71\n","0.0    59\n","2.0    48\n","Name: count, dtype: int64\n","\n","Decision Tree Classifier:\n","Accuracy: 0.9444444444444444\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.93      0.93        14\n","         1.0       0.93      1.00      0.97        14\n","         2.0       1.00      0.88      0.93         8\n","\n","    accuracy                           0.94        36\n","   macro avg       0.95      0.93      0.94        36\n","weighted avg       0.95      0.94      0.94        36\n","\n","[[13  1  0]\n"," [ 0 14  0]\n"," [ 1  0  7]]\n","\n","Random Forest Classifier:\n","Accuracy: 1.0\n","              precision    recall  f1-score   support\n","\n","         0.0       1.00      1.00      1.00        14\n","         1.0       1.00      1.00      1.00        14\n","         2.0       1.00      1.00      1.00         8\n","\n","    accuracy                           1.00        36\n","   macro avg       1.00      1.00      1.00        36\n","weighted avg       1.00      1.00      1.00        36\n","\n","[[14  0  0]\n"," [ 0 14  0]\n"," [ 0  0  8]]\n","\n","Model Comparison:\n","Random Forest performed slightly better (or equally).\n"]}]}]}